{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T20:46:13.301011Z",
     "start_time": "2020-07-31T20:46:12.784000Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# including the project directory to the notebook level\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "# import modules\n",
    "from alumni_scripts import data_process as dp\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T18:06:59.581490Z",
     "start_time": "2020-06-26T18:06:59.131895Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# run a single update of the data pull\n",
    "with open('../auths.json', 'r') as fp:\n",
    "    api_args = json.load(fp)\n",
    "\n",
    "time_args = {\n",
    "    'start_year': 2020,'start_month': 6,'start_day': 26,'start_hour': 12,'start_minute': 34,'start_second': 23,\n",
    "    'end_year':   2020,'end_month'  : 6,'end_day'  : 26,'end_hour'  : 13,'end_minute'  :  4,'end_second'  : 23,\n",
    "    'trend_id': '2681',\n",
    "    'save_path':'../data/raw_data/alumni_data.csv'\n",
    "}\n",
    "api_args.update(time_args)\n",
    "\n",
    "dp.pull_offline_data(**api_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# download data in a loop\n",
    "time_args = [\n",
    "    {\n",
    "    'start_year': 2018,'start_month': 7,'start_day': 1,'start_hour': 0,'start_minute': 0,'start_second': 0,\n",
    "    'end_year': 2018,'end_month': 12,'end_day': 31,'end_hour': 23,'end_minute': 59,'end_second': 59,\n",
    "    'trend_id': '2681',\n",
    "    'save_path':'../data/raw_data/alumni_data_jul2dec2018.csv'\n",
    "    },\n",
    "    {\n",
    "    'start_year': 2019,'start_month': 1,'start_day': 1,'start_hour': 0,'start_minute': 0,'start_second': 0,\n",
    "    'end_year': 2019,'end_month': 6,'end_day': 30,'end_hour': 23,'end_minute': 59,'end_second': 59,\n",
    "    'trend_id': '2681',\n",
    "    'save_path':'../data/raw_data/alumni_data_jan2jun2019.csv'  \n",
    "    },\n",
    "    {\n",
    "    'start_year': 2019,'start_month': 7,'start_day': 1,'start_hour': 0,'start_minute': 0,'start_second': 0,\n",
    "    'end_year': 2019,'end_month': 12,'end_day': 31,'end_hour': 23,'end_minute': 59,'end_second': 59,\n",
    "    'trend_id': '2681',\n",
    "    'save_path':'../data/raw_data/alumni_data_jul2dec2019.csv'  \n",
    "    },\n",
    "    {\n",
    "    'start_year': 2020,'start_month': 1,'start_day': 1,'start_hour': 0,'start_minute': 0,'start_second': 0,\n",
    "    'end_year': 2020,'end_month': 6,'end_day': 15,'end_hour': 23,'end_minute': 59,'end_second': 59,\n",
    "    'trend_id': '2681',\n",
    "    'save_path':'../data/raw_data/alumni_data_jan2jun2020.csv'  \n",
    "    }\n",
    "]\n",
    "for i in time_args:\n",
    "    with open('../auths.json', 'r') as fp:\n",
    "        api_args = json.load(fp)\n",
    "    api_args.update(i)\n",
    "    dp.pull_offline_data(**api_args)\n",
    "    print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Deployment testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T18:48:38.178110Z",
     "start_time": "2020-06-26T18:48:34.514080Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# testing the deploy control thread\n",
    "\"\"\" change path in the deploy control thread to save the file to appropriate location\"\"\"\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    from alumni_scripts import deploy_control as dc\n",
    "\n",
    "with open('../auths.json', 'r') as fp:\n",
    "    api_args = json.load(fp)\n",
    "with open('../alumni_scripts/meta_data.json', 'r') as fp:\n",
    "    meta_data_ = json.load(fp)\n",
    "obs_space_vars = ['oat', 'oah', 'wbt', 'avg_stpt', 'sat']\n",
    "\n",
    "df = dc.get_real_obs(api_args, meta_data_, obs_space_vars)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create Offline Batch Time Series Data Base for Alumni Hall data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T21:46:31.881844Z",
     "start_time": "2020-07-31T21:46:30.883094Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# including the project directory to the notebook level\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "# import modules\n",
    "from alumni_scripts import data_process as dp\n",
    "from alumni_scripts import alumni_data_utils as a_utils\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from influxdb import DataFrameClient\n",
    "from collections import OrderedDict\n",
    "from CoolProp.HumidAirProp import HAPropsSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Collate the data from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T15:29:51.933760Z",
     "start_time": "2020-06-29T15:29:51.718809Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# collate batch of data\n",
    "file_names = ['jul2dec2018', 'jan2jun2019', 'jul2dec2019', 'jan2jun2020']\n",
    "dflist = []\n",
    "for fname in file_names:\n",
    "    df_ = pd.read_csv('../data/raw_data/alumni_data_{}.csv'.format(fname))\n",
    "    df_['time'] = pd.to_datetime(df_['time'])\n",
    "    df_.set_index(keys='time',inplace=True, drop = True)\n",
    "    dflist.append(df_)\n",
    "df = a_utils.mergerows(dflist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Calculate Wet Bulb Temperature and add it to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T15:30:03.167935Z",
     "start_time": "2020-06-29T15:30:03.158776Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rh = df['WeatherDataProfile humidity']/100\n",
    "rh = rh.to_numpy()\n",
    "t_db = 5*(df['AHU_1 outdoorAirTemp']-32)/9 + 273.15\n",
    "t_db = t_db.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T17:16:22.674946Z",
     "start_time": "2020-06-21T17:00:06.936402Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "T = HAPropsSI('T_wb','R',rh,'T',t_db,'P',101325)\n",
    "t_f = 9*(T-273.15)/5 + 32\n",
    "df['wbt'] = t_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Create meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T17:19:07.937617Z",
     "start_time": "2020-06-21T17:19:07.687429Z"
    },
    "hidden": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# Create column_aliases\n",
    "d1 = {'column_names': list(df.columns)}\n",
    "column_aliases = [\n",
    "    'pchwst', 'vrf50', 'oat', 'sat', 'sat_stpt', 'oah', 'vrf67', 'pchw_flow',\n",
    "    'hwe', 'vrf1', 'vrf30', 'vrf34', 'vrf74', 'cwe', 'hws_st_stpt', 'vrf60',\n",
    "    'vrf63', 'hws_st', 'hws_vlv1', 'vrf77', 'vrf64', 'vrf10', 'ee', 'hws_rt',\n",
    "    'vrf100', 'vrf40', 'hws_flow', 'vrf108', 'vrf20', 'wbt'\n",
    "]\n",
    "\n",
    "d1['column_agg_type'] = {\n",
    "    \"pchwst\": \"mean\",\"vrf50\": \"mean\",\"oat\": \"mean\",\"sat\": \"mean\",\"sat_stpt\": \"mean\",\"oah\": \"mean\",\n",
    "    \"vrf67\": \"mean\",\"pchw_flow\": \"sum\",\"hwe\": \"sum\",\"vrf1\": \"mean\",\"vrf30\": \"mean\",\"vrf34\": \"mean\",\n",
    "    \"vrf74\": \"mean\",\"cwe\": \"sum\",\"hws_st_stpt\": \"mean\",\"vrf60\": \"mean\",\"vrf63\": \"mean\",\"hws_st\": \"mean\",\n",
    "    \"hws_vlv1\": \"sum\",\"vrf77\": \"mean\", \"vrf64\": \"mean\",\"vrf10\": \"mean\",\"ee\": \"sum\",\"hws_rt\": \"mean\",\n",
    "    \"vrf100\": \"mean\",\"vrf40\": \"mean\",\"hws_flow\": \"sum\",\"vrf108\": \"mean\",\"vrf20\": \"mean\", 'wbt' : \"mean\"\n",
    "}\n",
    "\n",
    "# Create column alias\n",
    "d2 = OrderedDict()\n",
    "for i, j in zip(df.columns, column_aliases):\n",
    "    d2.update({j: i})\n",
    "d1['column_aliases'] = d2\n",
    "\n",
    "df.columns = column_aliases\n",
    "\n",
    "# Create column stats\n",
    "stats = {}\n",
    "d3 = OrderedDict(df.describe())\n",
    "for key, alias in zip(d3.keys(), column_aliases):\n",
    "    stats[alias] = dict(d3[key])\n",
    "d1['column_stats'] = stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Create half hour stats for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T17:19:11.453611Z",
     "start_time": "2020-06-21T17:19:11.167137Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# aggregate data\n",
    "rolling_sum_target, rolling_mean_target = [], []\n",
    "for key, value in d1['column_agg_type'].items():\n",
    "    if value == 'sum': rolling_sum_target.append(key)\n",
    "    else: rolling_mean_target.append(key)\n",
    "\n",
    "df_agg = df.copy()\n",
    "        \n",
    "df_agg[rolling_sum_target] =  a_utils.window_sum(df_agg, window_size=6, column_names=rolling_sum_target)\n",
    "df_agg[rolling_mean_target] =  a_utils.window_mean(df_agg, window_size=6, column_names=rolling_mean_target)\n",
    "df_agg = a_utils.dropNaNrows(df_agg)\n",
    "# sample at half hour\n",
    "df_agg = a_utils.sample_timeseries_df(df_agg, period=6)\n",
    "\n",
    "# Create column stats for half hour data\n",
    "stats_halfhour = {}\n",
    "d4 = OrderedDict(df_agg.describe())\n",
    "for key, alias in zip(d4.keys(),column_aliases):\n",
    "    stats_halfhour[alias] = dict(d4[key])\n",
    "d1['column_stats_half_hour'] = stats_halfhour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T17:19:13.689909Z",
     "start_time": "2020-06-21T17:19:13.680430Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create meta data json file\n",
    "with open('../alumni_scripts/meta_data.json', 'w') as fp:\n",
    "    json.dump(d1, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T17:19:29.004848Z",
     "start_time": "2020-06-21T17:19:28.836280Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_cleaned = dp.offline_batch_data_clean(meta_data_path='../alumni_scripts/meta_data.json', df = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T17:19:37.939990Z",
     "start_time": "2020-06-21T17:19:37.935222Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_cleaned.columns = column_aliases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Push data to a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T17:20:39.849700Z",
     "start_time": "2020-06-21T17:20:29.594431Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "before the next steps launch influxd client at a cli\n",
    "sudo influxd\n",
    "\"\"\"\n",
    "# launch python client for influxdb\n",
    "client = DataFrameClient(host='localhost', port=8086)\n",
    "# create a database inc case it's not there\n",
    "client.create_database('bdx_batch_db')\n",
    "# get list of database\n",
    "client.get_list_database()\n",
    "# switch to the databaase you want\n",
    "client.switch_database('bdx_batch_db')\n",
    "# write \"dataframe\" as \"measurements\"\n",
    "client.write_points(dataframe=df_cleaned,\n",
    "                    measurement='alumni_data_v2',\n",
    "                    tags={\n",
    "                        'data_cleaned': 'True',\n",
    "                        'aggregated': False,\n",
    "                        'time-interval': '5 minutes'\n",
    "                    },\n",
    "                    protocol='line',\n",
    "                    batch_size=5000)\n",
    "# see measurement added to curent db\n",
    "client.get_list_measurements()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Read data from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T17:21:45.388287Z",
     "start_time": "2020-06-21T17:21:44.761226Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "before the next steps launch influxd client at a cli\n",
    "sudo influxd\n",
    "\"\"\"\n",
    "# launch python client for influxdb\n",
    "client = DataFrameClient(host='localhost', port=8086)\n",
    "# switch to the databaase you want\n",
    "client.switch_database('bdx_batch_db')\n",
    "results_obj = client.query(\n",
    "    \"select * from alumni_data_v2 \\\n",
    "    where time >= '2018-11-15 12:25:00' - 13w \\\n",
    "    and time < '2018-11-15 12:25:00'\"\n",
    ")\n",
    "df2 = results_obj['alumni_data_v2']\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Read the data from the data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T17:53:08.213080Z",
     "start_time": "2020-06-22T17:53:06.664023Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# including the project directory to the notebook level\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "# import modules\n",
    "from alumni_scripts import data_process as dp\n",
    "from alumni_scripts import alumni_data_utils as utils\n",
    "import json\n",
    "import pandas as pd\n",
    "from influxdb import DataFrameClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T18:10:44.190923Z",
     "start_time": "2020-06-22T18:10:38.649245Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "before the next steps launch influxd client at a cli\n",
    "sudo influxd\n",
    "\"\"\"\n",
    "# launch python client for influxdb\n",
    "client = DataFrameClient(host='localhost', port=8086)\n",
    "# switch to the databaase you want\n",
    "client.switch_database('bdx_batch_db')\n",
    "results_obj = client.query(\n",
    "    \"select * from alumni_data_v2\\\n",
    "    where time >= '2018-08-07 00:00:00' \\\n",
    "    and time < '2019-02-07 00:00:00'\" \n",
    ")\n",
    "df2 = results_obj['alumni_data_v2']\n",
    "df2.drop(columns=['aggregated','data_cleaned','time-interval'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create meta_data : Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:09:33.573019Z",
     "start_time": "2020-06-21T16:09:33.260632Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# including the project directory to the notebook level\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "# import modules\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Create column_aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw_data/alumni_data_jul2dec2018.csv', index_col='time')\n",
    "\n",
    "d1 = {'column_names': list(df.columns)}\n",
    "column_aliases = [\n",
    "    'pchwst', 'vrf50', 'oat', 'sat', 'sat_stpt', 'oah', 'vrf67', 'pchw_flow',\n",
    "    'hwe', 'vrf1', 'vrf30', 'vrf34', 'vrf74', 'cwe', 'hws_st_stpt', 'vrf60',\n",
    "    'vrf63', 'hws_st', 'hws_vlv1', 'vrf77', 'vrf64', 'vrf10', 'ee', 'hws_rt',\n",
    "    'vrf100', 'vrf40', 'hws_flow', 'vrf108', 'vrf20'\n",
    "]\n",
    "\n",
    "d2 = {}\n",
    "for i, j in zip(df.columns, column_aliases):\n",
    "    d2.update({j:i})\n",
    "d1['column_aliases'] = d2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Create column stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stats = {}\n",
    "d3 = dict(df.describe())\n",
    "for key in d3.keys():\n",
    "    stats[key] = dict(d3[key])\n",
    "d1['column_stats'] = stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Dump meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('../alumni_scripts/meta_data.json', 'w') as fp:\n",
    "    json.dump(d1, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Read meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('../alumni_scripts/meta_data.json', 'r') as fp:\n",
    "        meta_data_ = json.load(fp)\n",
    "meta_data = meta_data_.copy()\n",
    "for key, value in meta_data_['column_stats'].items():\n",
    "    if value['std'] == 0:\n",
    "        meta_data['column_stats'][key]['std'] = 0.0001  # add small std for constant values\n",
    "stats = pd.DataFrame(meta_data['column_stats'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Plot data before and after cleaning: Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T21:24:58.464354Z",
     "start_time": "2020-06-27T21:24:58.457319Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# including the project directory to the notebook level\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "# import modules\n",
    "from alumni_scripts import data_process as dp\n",
    "from alumni_scripts import alumni_data_utils as a_utils\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from collections import OrderedDict\n",
    "from CoolProp.HumidAirProp import HAPropsSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T21:25:00.973908Z",
     "start_time": "2020-06-27T21:25:00.213527Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# collate batch of data\n",
    "file_names = ['jul2dec2018', 'jan2jun2019', 'jul2dec2019', 'jan2jun2020']\n",
    "dflist = []\n",
    "for fname in file_names:\n",
    "    df_ = pd.read_csv('../data/raw_data/alumni_data_{}.csv'.format(fname))\n",
    "    df_['time'] = pd.to_datetime(df_['time'])\n",
    "    df_.set_index(keys='time',inplace=True, drop = True)\n",
    "    dflist.append(df_)\n",
    "df = a_utils.mergerows(dflist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T21:25:07.126472Z",
     "start_time": "2020-06-27T21:25:07.115986Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rh = df['WeatherDataProfile humidity']/100\n",
    "rh = rh.to_numpy()\n",
    "t_db = 5*(df['AHU_1 outdoorAirTemp']-32)/9 + 273.15\n",
    "t_db = t_db.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T21:41:31.154129Z",
     "start_time": "2020-06-27T21:25:12.798214Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "T = HAPropsSI('T_wb','R',rh,'T',t_db,'P',101325)\n",
    "t_f = 9*(T-273.15)/5 + 32\n",
    "df['wbt'] = t_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T21:41:39.084300Z",
     "start_time": "2020-06-27T21:41:39.078568Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "column_aliases = [\n",
    "    'pchwst', 'vrf50', 'oat', 'sat', 'sat_stpt', 'oah', 'vrf67', 'pchw_flow',\n",
    "    'hwe', 'vrf1', 'vrf30', 'vrf34', 'vrf74', 'cwe', 'hws_st_stpt', 'vrf60',\n",
    "    'vrf63', 'hws_st', 'hws_vlv1', 'vrf77', 'vrf64', 'vrf10', 'ee', 'hws_rt',\n",
    "    'vrf100', 'vrf40', 'hws_flow', 'vrf108', 'vrf20', 'wbt'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T21:42:02.128280Z",
     "start_time": "2020-06-27T21:42:02.123146Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.columns = column_aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T21:45:00.901390Z",
     "start_time": "2020-06-27T21:45:00.708715Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('../alumni_scripts/meta_data.json', 'r') as fp:\n",
    "        meta_data_ = json.load(fp)\n",
    "df_cleaned1 = dp.offline_batch_data_clean(meta_data_=meta_data_, df = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T22:07:24.569559Z",
     "start_time": "2020-06-27T22:07:24.337523Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stats = {}\n",
    "d2 = dict(df_cleaned1.describe())\n",
    "for key in d2.keys():\n",
    "    stats[key] = dict(d2[key])\n",
    "d1 ={'column_stats' : stats}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T22:07:28.273294Z",
     "start_time": "2020-06-27T22:07:27.996903Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d3 = {'column_agg_type':\n",
    "    {\"pchwst\": \"mean\",\"vrf50\": \"mean\",\"oat\": \"mean\",\"sat\": \"mean\",\"sat_stpt\": \"mean\",\"oah\": \"mean\",\n",
    "    \"vrf67\": \"mean\",\"pchw_flow\": \"sum\",\"hwe\": \"sum\",\"vrf1\": \"mean\",\"vrf30\": \"mean\",\"vrf34\": \"mean\",\n",
    "    \"vrf74\": \"mean\",\"cwe\": \"sum\",\"hws_st_stpt\": \"mean\",\"vrf60\": \"mean\",\"vrf63\": \"mean\",\"hws_st\": \"mean\",\n",
    "    \"hws_vlv1\": \"sum\",\"vrf77\": \"mean\", \"vrf64\": \"mean\",\"vrf10\": \"mean\",\"ee\": \"sum\",\"hws_rt\": \"mean\",\n",
    "    \"vrf100\": \"mean\",\"vrf40\": \"mean\",\"hws_flow\": \"sum\",\"vrf108\": \"mean\",\"vrf20\": \"mean\", 'wbt' : \"mean\"\n",
    "}}\n",
    "\n",
    "# aggregate data\n",
    "rolling_sum_target, rolling_mean_target = [], []\n",
    "for key, value in d3['column_agg_type'].items():\n",
    "    if value == 'sum': rolling_sum_target.append(key)\n",
    "    else: rolling_mean_target.append(key)\n",
    "\n",
    "df_agg = df_cleaned1.copy()\n",
    "        \n",
    "df_agg[rolling_sum_target] =  a_utils.window_sum(df_agg, window_size=6, column_names=rolling_sum_target)\n",
    "df_agg[rolling_mean_target] =  a_utils.window_mean(df_agg, window_size=6, column_names=rolling_mean_target)\n",
    "df_agg = a_utils.dropNaNrows(df_agg)\n",
    "# sample at half hour\n",
    "df_agg = a_utils.sample_timeseries_df(df_agg, period=6)\n",
    "\n",
    "# Create column stats for half hour data\n",
    "stats_halfhour = {}\n",
    "d4 = OrderedDict(df_agg.describe())\n",
    "for key, alias in zip(d4.keys(),column_aliases):\n",
    "    stats_halfhour[alias] = dict(d4[key])\n",
    "d1['column_stats_half_hour'] = stats_halfhour\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T22:07:59.003757Z",
     "start_time": "2020-06-27T22:07:58.994792Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('../alumni_scripts/meta_data1.json', 'w') as fp:\n",
    "    json.dump(d1, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col_name in df.columns:\n",
    "    utils.dataframeplot(df[[col_name]],lazy=False,legend=True)\n",
    "    utils.dataframeplot(df_cleaned[[col_name]],lazy=False,legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "scrolled": false
   },
   "source": [
    "## Create Time Series Data Base: Demo only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T20:24:14.854873Z",
     "start_time": "2020-06-19T20:24:13.316916Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# including the project directory to the notebook level\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "# import modules\n",
    "from alumni_scripts import data_process as dp\n",
    "from alumni_scripts import alumni_data_utils as utils\n",
    "import json\n",
    "import pandas as pd\n",
    "from influxdb import DataFrameClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw_data/alumni_data_jul2dec2018.csv',)\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df.set_index(keys='time',inplace=True, drop = True)\n",
    "df_cleaned = dp.offline_batch_data_clean(meta_data_path='../alumni_scripts/meta_data.json', df = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "before the next steps launch influxd client at a cli\n",
    "sudo influxd\n",
    "\"\"\"\n",
    "# launch python client for influxdb\n",
    "client = DataFrameClient(host='localhost', port=8086)\n",
    "# create a database inc case it's not there\n",
    "client.create_database('demo_alumni')\n",
    "# get list of database\n",
    "client.get_list_database()\n",
    "# switch to the databaase you want\n",
    "client.switch_database('demo_alumni')\n",
    "# write \"dataframe\" as \"measurements\"\n",
    "client.write_points(dataframe=df_cleaned, measurement='alumni_jul2dec2018', protocol='line', batch_size=5000)\n",
    "# see measurement added to curent db\n",
    "client.get_list_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_obj = client.query(\n",
    "    \"select * from alumni_jul2dec2018 where time >= '2018-11-15 12:00:00' and time < '2018-11-15 12:05:00'\"\n",
    ")\n",
    "df2 = results_obj['alumni_jul2dec2018']\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "results_obj2 = client.query(\n",
    "    \"select * from alumni_jul2dec2018 where time = '2018-11-15 12:00:00'\"\n",
    ")\n",
    "df3 = results_obj2['alumni_jul2dec2018']\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# drop the database after the demo\n",
    "client.drop_database('demo_alumni')\n",
    "client.get_list_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# close client\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Code cemetery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T22:22:24.658879Z",
     "start_time": "2020-06-20T22:22:24.649558Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "time_str = '2018-11-15 13:12:00'\n",
    "from datetime import datetime, timedelta\n",
    "time_now = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')\n",
    "print(time_now)\n",
    "time_now_str = str(time_now)\n",
    "print(time_now_str)\n",
    "time_now ==time_now_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-21T16:17:03.354684Z",
     "start_time": "2020-06-21T16:17:03.349072Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('../alumni_scripts/meta_data.json', 'r') as fp:\n",
    "        meta_data_ = json.load(fp)\n",
    "meta_data_['column_agg_type']['pchwst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T18:09:27.277220Z",
     "start_time": "2020-06-20T18:09:27.271113Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "meta_data_['column_agg_type'].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T18:15:04.955338Z",
     "start_time": "2020-06-20T18:15:04.949929Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rolling_sum_target = []\n",
    "rolling_mean_target = []\n",
    "for key, value in meta_data_['column_agg_type'].items():\n",
    "    if value == 'sum': rolling_sum_target.append(key)\n",
    "    else: rolling_mean_target.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-20T18:16:14.744650Z",
     "start_time": "2020-06-20T18:16:14.738769Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rolling_sum_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "q = {'c': '11', 'b' : ['1f3','a']}\n",
    "with open('../logs/cwe_test_info.txt', 'a') as ifile:\n",
    "        ifile.write(json.dumps(q)+'\\n',)      \n",
    "with open('../logs/cwe_test_info.txt') as f:\n",
    "    for line in f:\n",
    "        document = json.loads(line)\n",
    "        print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T21:46:44.327977Z",
     "start_time": "2020-07-31T21:46:44.323491Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from CoolProp.HumidAirProp import HAPropsSI\n",
    "from dateutil import tz\n",
    "\n",
    "df_ = pd.read_csv('../data/trend_data/alumni_data_train.csv', )\n",
    "df_['time'] = pd.to_datetime(df_['time'])\n",
    "to_zone = tz.tzlocal()\n",
    "df_['time'] = df_['time'].apply(lambda x: x.astimezone(to_zone)) # convert time to loca timezones\n",
    "df_.set_index(keys='time',inplace=True, drop = True)\n",
    "df_ = a_utils.dropNaNrows(df_)\n",
    "\n",
    "rh = df_['WeatherDataProfile humidity']/100\n",
    "rh = rh.to_numpy()\n",
    "t_db = 5*(df_['AHU_1 outdoorAirTemp']-32)/9 + 273.15\n",
    "t_db = t_db.to_numpy()\n",
    "\n",
    "tdb_rh = np.concatenate((t_db.reshape(-1,1), rh.reshape(-1,1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T21:51:53.372663Z",
     "start_time": "2020-07-31T21:51:53.365208Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import psutil\n",
    "chunks = [\n",
    "    (sub_arr[:, 0].flatten(), sub_arr[:, 1].flatten(), cpu_id)\n",
    "    for cpu_id, sub_arr in enumerate(np.array_split(tdb_rh, multiprocessing.cpu_count(), axis=0))]\n",
    "\n",
    "def unpacking_apply_along_axis(all_args):\n",
    "    t_db, rh, cpu_id = all_args\n",
    "    \n",
    "    proc = psutil.Process()\n",
    "    proc.cpu_affinity([cpu_id])\n",
    "    \n",
    "    T = HAPropsSI('T_wb','R',rh,'T',t_db,'P',101325)\n",
    "    return T\n",
    "\n",
    "pool = multiprocessing.Pool()\n",
    "individual_results = pool.map(unpacking_apply_along_axis, chunks)\n",
    "# Freeing the workers:\n",
    "pool.close()\n",
    "pool.join()\n",
    "final_T = np.concatenate(individual_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotly example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T17:07:29.575413Z",
     "start_time": "2020-08-01T17:07:28.300253Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T17:31:08.908739Z",
     "start_time": "2020-08-01T17:31:08.859039Z"
    }
   },
   "outputs": [],
   "source": [
    "basepath='../tmp/cwe_data/cwe'\n",
    "labelnames = ['sat-oat', 'oah', 'wbt', 'pchw_flow', 'cwe']\n",
    "X_train = np.load(basepath+'_X_train.npy')\n",
    "y_train = np.load(basepath+'_y_train.npy')\n",
    "X_val = np.load(basepath+'_X_val.npy')\n",
    "y_val = np.load(basepath+'_y_val.npy')\n",
    "\n",
    "train = np.concatenate((X_train, y_train), axis=-1)[:,0,:]\n",
    "\n",
    "fig = go.Figure()\n",
    "for i in range(train.shape[1]):\n",
    "    fig.add_trace(go.Scatter(y=train[:, i], mode='lines', name=labelnames[i]))\n",
    "\n",
    "# Edit the layout\n",
    "fig.update_layout(title='Different Variables',\n",
    "                  xaxis_title='Time Points',\n",
    "                  yaxis_title='Scaled Values',\n",
    "                  font = {'family':'Times New Roman', 'size': 15})\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T17:31:34.622151Z",
     "start_time": "2020-08-01T17:31:34.573219Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "basepath='../tmp/hwe_data/hwe'\n",
    "labelnames = ['oat', 'oah', 'wbt', 'sat-oat', 'hwe']\n",
    "X_train = np.load(basepath+'_X_train.npy')\n",
    "y_train = np.load(basepath+'_y_train.npy')\n",
    "X_val = np.load(basepath+'_X_val.npy')\n",
    "y_val = np.load(basepath+'_y_val.npy')\n",
    "\n",
    "train = np.concatenate((X_train, y_train), axis=-1)[:,0,:]\n",
    "\n",
    "fig = go.Figure()\n",
    "for i in range(train.shape[1]):\n",
    "    fig.add_trace(go.Scatter(y=train[:, i], mode='lines', name=labelnames[i]))\n",
    "\n",
    "# Edit the layout\n",
    "fig.update_layout(title='Different Variables',\n",
    "                  xaxis_title='Time Points',\n",
    "                  yaxis_title='Scaled Values',\n",
    "                  font = {'family':'Times New Roman', 'size': 15})\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T17:31:56.393633Z",
     "start_time": "2020-08-01T17:31:56.195464Z"
    }
   },
   "outputs": [],
   "source": [
    "basepath='../tmp/vlv_data/vlv'\n",
    "labelnames = ['oat', 'oah', 'wbt', 'sat-oat', 'vlv_off', 'vlv_on']\n",
    "X_train = np.load(basepath+'_X_train.npy')\n",
    "y_train = np.load(basepath+'_y_train.npy')\n",
    "X_val = np.load(basepath+'_X_val.npy')\n",
    "y_val = np.load(basepath+'_y_val.npy')\n",
    "\n",
    "train = np.concatenate((X_train, y_train), axis=-1)[:,0,:]\n",
    "\n",
    "fig = go.Figure()\n",
    "for i in range(train.shape[1]):\n",
    "    fig.add_trace(go.Scatter(y=train[:, i], mode='lines', name=labelnames[i]))\n",
    "\n",
    "# Edit the layout\n",
    "fig.update_layout(title='Different Variables',\n",
    "                  xaxis_title='Time Points',\n",
    "                  yaxis_title='Scaled Values',\n",
    "                  font = {'family':'Times New Roman', 'size': 15})\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('sbvenv1': venv)",
   "language": "python",
   "name": "python36964bitsbvenv1venvf8f19bb32ed941ad82d727bac6ccbb65"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "278px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
